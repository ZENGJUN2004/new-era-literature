import requests
from bs4 import BeautifulSoup
import json
import time
import random
import sys
import io
import datetime

# 1. åŸºç¡€è®¾ç½®
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

def get_header():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.baidu.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }

# 2. æ™ºèƒ½åˆ†ç±»å™¨ (ä¿æŒä¸å˜)
def auto_classify(title):
    t = title.lower()
    if any(k in t for k in ['ç ”è®¨', 'åº§è°ˆ', 'è®ºå›', 'å³°ä¼š', 'å¹´ä¼š', 'ä¼šè®®', 'è‡´è¾', 'å¼€å¹•', 'è®²åº§']):
        return 'meeting'
    if any(k in t for k in ['ä¸“è®¿', 'å¯¹è¯', 'è°ˆ', 'è¯´', 'è®º', 'åºè¨€', 'è¯»åæ„Ÿ', 'æ‰¹è¯„', 'è§‚ç‚¹', 'ç»¼è¿°', 'ç¬”è°ˆ']):
        return 'voice'
    if any(k in t for k in ['å‘å¸ƒ', 'æ­æ™“', 'é¢å¥–', 'å¯åŠ¨', 'å¾æ–‡', 'å¤§èµ›', 'æ´»åŠ¨', 'ç›®å½•', 'å¾ç¨¿']):
        return 'activity'
    return 'other'

# ==========================================
# 3. æ ¸å¿ƒå‡çº§ï¼šå››å¤§æœç´¢æˆ˜åŒº
# ==========================================
SEARCH_ZONES = [
    # æˆ˜åŒºAï¼šä½œåä¸å®˜æ–¹ (ç›¯ç€ä¸­å›½ä½œå®¶ç½‘ã€å„çœä½œå)
    {
        "name": "ä½œååŠ¨æ€",
        "keywords": ["ä¸­å›½ä½œå", "é²è¿…æ–‡å­¦å¥–", "èŒ…ç›¾æ–‡å­¦å¥–", "ä½œå ç ”è®¨ä¼š", "ä½œå®¶åä¼š å…¬ç¤º"],
        "extra_query": " site:chinawriter.com.cn" # ä¸“é—¨æœä¸­å›½ä½œå®¶ç½‘
    },
    # æˆ˜åŒºBï¼šæœŸåˆŠä¸å‡ºç‰ˆ (ç›¯ç€å„å¤§æ–‡å­¦æœŸåˆŠã€æ–°ä¹¦)
    {
        "name": "æœŸåˆŠå‡ºç‰ˆ",
        "keywords": ["æ–‡å­¦æœŸåˆŠ ç›®å½•", "é•¿ç¯‡å°è¯´é€‰åˆŠ", "æ”¶è·æ‚å¿—", "äººæ°‘æ–‡å­¦", "å½“ä»£ä½œå®¶è¯„è®º", "æ–°ä¹¦å‘å¸ƒä¼š", "æ–‡å­¦å¾æ–‡"],
        "extra_query": "" 
    },
    # æˆ˜åŒºCï¼šé«˜æ ¡ä¸å­¦æœ¯ (ç›¯ç€ edu.cn åç¼€çš„å¤§å­¦ç½‘ç«™)
    {
        "name": "é«˜æ ¡å­¦æœ¯",
        "keywords": ["ä¸­æ–‡ç³» è®²åº§", "æ–‡å­¦é™¢ ä¼šè®®", "æ¯”è¾ƒæ–‡å­¦ è®ºå›", "æ•°å­—äººæ–‡ ç ”è®¨", "åˆ›æ„å†™ä½œ"],
        "extra_query": " site:edu.cn" # å¿…æ€æŠ€ï¼šåªæœå¤§å­¦ç½‘ç«™
    },
    # æˆ˜åŒºDï¼šå¾®ä¿¡ä¸ç½‘ç»œ (æœç‹—å¾®ä¿¡å¾ˆéš¾çˆ¬ï¼Œæˆ‘ä»¬ç”¨ç™¾åº¦æœèšåˆå†…å®¹)
    {
        "name": "ç½‘ç»œçƒ­ç‚¹",
        "keywords": ["æ–‡å­¦è¯„è®º å¾®ä¿¡å…¬ä¼—å·", "ä½œå®¶ä¸“è®¿ æ·±åº¦", "è±†ç“£è¯»ä¹¦ é«˜åˆ†", "ç½‘ç»œæ–‡å­¦ æ’è¡Œæ¦œ"],
        "extra_query": ""
    }
]

def fetch_zone_news(zone):
    print(f"\n>>> æ­£åœ¨æ‰«ææˆ˜åŒºï¼š[{zone['name']}] ...")
    zone_pool = []
    
    for kw in zone['keywords']:
        # ç»„åˆæœç´¢è¯ï¼šå…³é”®è¯ + é™å®šç½‘ç«™
        # ä¾‹å¦‚ï¼š"ä¸­æ–‡ç³» è®²åº§ site:edu.cn"
        query = kw + zone['extra_query']
        print(f"  - æœç´¢æŒ‡ä»¤: {query}")
        
        url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd={query}"
        
        try:
            res = requests.get(url, headers=get_header(), timeout=12)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            items = soup.find_all('div', class_='result-op')
            if not items: items = soup.find_all('div', class_='result')
            
            count = 0
            for item in items:
                try:
                    title_tag = item.find('h3').find('a')
                    title = title_tag.get_text(strip=True)
                    link = title_tag['href']
                    
                    # è·å–æ¥æº
                    source_node = item.find('span', class_='c-color-gray')
                    source = source_node.get_text(strip=True) if source_node else zone['name']
                    
                    # è·å–æ—¶é—´
                    time_node = item.find('span', class_='c-color-gray2')
                    pub_time = time_node.get_text(strip=True) if time_node else "è¿‘æœŸ"
                    
                    category = auto_classify(title)
                    
                    # å»é‡é€»è¾‘
                    if not any(n['title'] == title for n in zone_pool):
                        zone_pool.append({
                            "title": title, "url": link, "source": source, 
                            "time": pub_time, "category": category
                        })
                        count += 1
                except:
                    continue
            # print(f"    æ‰¾åˆ° {count} æ¡")
            time.sleep(1.5) # ç¨å¾®æ…¢ç‚¹ï¼Œé˜²æ­¢ç™¾åº¦å°é”
            
        except Exception as e:
            print(f"    æœç´¢æŠ¥é”™: {e}")

    return zone_pool[:15] # æ¯ä¸ªæˆ˜åŒºå–å‰15æ¡

def fetch_all():
    all_news = []
    
    # 1. å¾ªç¯æ‰«ææ‰€æœ‰æˆ˜åŒº
    for zone in SEARCH_ZONES:
        news = fetch_zone_news(zone)
        all_news.extend(news)
    
    # 2. æ’å…¥â€œç¡¬é“¾æ¥â€ï¼šé’ˆå¯¹å¾ˆéš¾çˆ¬å–çš„å¹³å°ï¼ˆçŸ¥ç½‘ã€å¾®ä¿¡ã€ç¤¾ç§‘ç½‘ï¼‰
    # ç›´æ¥æä¾›è·³è½¬é“¾æ¥ï¼Œè®©ç”¨æˆ·ç‚¹è¿‡å»çœ‹ï¼Œè¿™æ˜¯æœ€ç¨³å®šçš„
    print("\n>>> ç”Ÿæˆé™æ€ç›´è¾¾å…¥å£...")
    static_links = [
        {"title": "ğŸ‘‰ã€å¾®ä¿¡æœç‹—ã€‘ç‚¹å‡»æŸ¥çœ‹â€œæ–‡å­¦è¯„è®ºâ€å…¬ä¼—å·æœ€æ–°æ–‡ç« ", "url": "https://weixin.sogou.com/weixin?type=2&query=æ–‡å­¦è¯„è®º", "source": "å¾®ä¿¡çŸ©é˜µ", "time": "å®æ—¶", "category": "voice"},
        {"title": "ğŸ‘‰ã€ä¸­å›½ç¤¾ç§‘ç½‘ã€‘æ–‡å­¦ç†è®ºå‰æ²¿èµ„è®¯", "url": "http://lit.cssn.cn/wx/", "source": "CSSN", "time": "å®æ—¶", "category": "meeting"},
        {"title": "ğŸ‘‰ã€çŸ¥ç½‘ã€‘â€œæ•°å­—äººæ–‡â€æœ€æ–°å­¦æœ¯è®ºæ–‡(ç‚¹å‡»æŒ‰æ—¶é—´æ’åº)", "url": "https://scholar.baidu.com/scholar?q=æ•°å­—äººæ–‡&sc_ylo=2024&as_ylo=2025", "source": "ç™¾åº¦å­¦æœ¯", "time": "å®æ—¶", "category": "activity"},
    ]
    
    # 3. æ··åˆå¹¶å»é‡
    final_list = static_links + all_news
    
    # ç®€å•çš„æŒ‰æ ‡é¢˜å»é‡ï¼ˆé˜²æ­¢ä¸åŒå…³é”®è¯æœåˆ°åŒä¸€ç¯‡ï¼‰
    seen = set()
    unique_list = []
    for item in final_list:
        if item['title'] not in seen:
            unique_list.append(item)
            seen.add(item['title'])
            
    return unique_list

def save(data):
    try:
        # è°ƒæ•´ä¸ºåŒ—äº¬æ—¶é—´
        utc_now = datetime.datetime.utcnow()
        cst_now = utc_now + datetime.timedelta(hours=8)
        time_str = cst_now.strftime('%Y-%m-%d %H:%M')
        
        final_json = {
            "update_time": time_str,
            "news": data
        }
        
        # å†™å…¥æ–‡ä»¶
        with open("data.js", "w", encoding="utf-8") as f:
            f.write(f"window.LIT_DATA = {json.dumps(final_json, ensure_ascii=False, indent=2)};")
            
        print("-" * 30)
        print(f"âœ… æŠ“å–å®Œæˆï¼å…±æ”¶é›† {len(data)} æ¡æ•°æ®")
        print(f"æ—¶é—´å·²æ ¡å‡†ä¸º: {time_str}")
        
    except Exception as e:
        print(f"Save Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    data = fetch_all()
    save(data)
