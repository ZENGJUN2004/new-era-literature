import requests
from bs4 import BeautifulSoup
import json
import time
import random
import sys
import io
import datetime
import re

# 1. åŸºç¡€ç¯å¢ƒè®¾ç½®
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# --- æ ¸å¿ƒé…ç½®ï¼šåªä¿ç•™æœ€è¿‘ 48 å°æ—¶å†…çš„ä¿¡æ¯ ---
MAX_DAYS_AGO = 2 

def get_header():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.baidu.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }

# 2. æ™ºèƒ½æ—¶é—´è§£æå™¨ (æŠŠâ€œ5åˆ†é’Ÿå‰â€è½¬æ¢æˆç”µè„‘èƒ½æ‡‚çš„æ—¶é—´)
def parse_baidu_time(time_str):
    now = datetime.datetime.now()
    time_str = str(time_str).strip()
    try:
        if "åˆ†é’Ÿå‰" in time_str:
            mins = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(minutes=mins)
        elif "å°æ—¶å‰" in time_str:
            hours = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(hours=hours)
        elif "æ˜¨å¤©" in time_str:
            return now - datetime.timedelta(days=1)
        elif "å‰å¤©" in time_str:
            return now - datetime.timedelta(days=2)
        elif "å¤©å‰" in time_str:
            days = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(days=days)
        elif "å¹´" in time_str or "-" in time_str:
            # å¤„ç†æ ‡å‡†æ—¥æœŸæ ¼å¼
            clean_str = time_str.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            dt = datetime.datetime.strptime(clean_str, "%Y-%m-%d")
            # å¦‚æœæ²¡æœ‰å…·ä½“æ—¶é—´ï¼Œé»˜è®¤è®¾ä¸ºå½“å¤©çš„ 00:00
            return dt
        else:
            # é‡åˆ°â€œåˆšåˆšâ€æˆ–è€…æ— æ³•è¯†åˆ«çš„ï¼Œé»˜è®¤ç®—ä½œæœ€æ–°
            return now
    except:
        # å¦‚æœè§£æå¤±è´¥ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œç®—ä½œæ—§æ–°é—»æ‰”æ‰
        return now - datetime.timedelta(days=365)

# 3. æ™ºèƒ½åˆ†ç±» (ä¼šè®®/å£°éŸ³/æ´»åŠ¨)
def auto_classify(title):
    t = title.lower()
    if any(k in t for k in ['ç ”è®¨', 'åº§è°ˆ', 'è®ºå›', 'å³°ä¼š', 'å¹´ä¼š', 'ä¼šè®®', 'è‡´è¾', 'è®²åº§']):
        return 'meeting'
    if any(k in t for k in ['ä¸“è®¿', 'å¯¹è¯', 'è°ˆ', 'è¯´', 'è®º', 'åºè¨€', 'è¯»åæ„Ÿ', 'æ‰¹è¯„', 'è§‚ç‚¹', 'ç»¼è¿°', 'ç¬”è°ˆ']):
        return 'voice'
    if any(k in t for k in ['å‘å¸ƒ', 'æ­æ™“', 'é¢å¥–', 'å¯åŠ¨', 'å¾æ–‡', 'å¤§èµ›', 'æ´»åŠ¨', 'ç›®å½•', 'å¾ç¨¿']):
        return 'activity'
    return 'other'

# ==========================================
# 4. å››å¤§æœç´¢æˆ˜åŒº (è¦†ç›–ä½œåã€é«˜æ ¡ã€æœŸåˆŠã€å¾®ä¿¡)
# ==========================================
SEARCH_ZONES = [
    # æˆ˜åŒºAï¼šä½œåä¸å®˜æ–¹ (ç›¯ç€ä¸­å›½ä½œå®¶ç½‘ã€å„çœä½œå)
    {
        "name": "ä½œååŠ¨æ€",
        "keywords": ["ä¸­å›½ä½œå", "é²è¿…æ–‡å­¦å¥–", "èŒ…ç›¾æ–‡å­¦å¥–", "ä½œå ç ”è®¨ä¼š", "ä½œå®¶åä¼š å…¬ç¤º"],
        "extra_query": " site:chinawriter.com.cn" # å¿…æ€æŠ€ï¼šåªæœä¸­å›½ä½œå®¶ç½‘
    },
    # æˆ˜åŒºBï¼šæœŸåˆŠä¸å‡ºç‰ˆ (ç›¯ç€å„å¤§æ–‡å­¦æœŸåˆŠã€æ–°ä¹¦)
    {
        "name": "æœŸåˆŠå‡ºç‰ˆ",
        "keywords": ["æ–‡å­¦æœŸåˆŠ ç›®å½•", "é•¿ç¯‡å°è¯´é€‰åˆŠ", "æ”¶è·æ‚å¿—", "äººæ°‘æ–‡å­¦", "å½“ä»£ä½œå®¶è¯„è®º", "æ–°ä¹¦å‘å¸ƒä¼š"],
        "extra_query": "" 
    },
    # æˆ˜åŒºCï¼šé«˜æ ¡ä¸å­¦æœ¯ (ç›¯ç€å…¨å›½å¤§å­¦ç½‘ç«™)
    {
        "name": "é«˜æ ¡å­¦æœ¯",
        "keywords": ["ä¸­æ–‡ç³» è®²åº§", "æ–‡å­¦é™¢ ä¼šè®®", "æ¯”è¾ƒæ–‡å­¦ è®ºå›", "æ•°å­—äººæ–‡ ç ”è®¨", "åˆ›æ„å†™ä½œ"],
        "extra_query": " site:edu.cn" # å¿…æ€æŠ€ï¼šåªæœ .edu.cn ç»“å°¾çš„å¤§å­¦å®˜ç½‘
    },
    # æˆ˜åŒºDï¼šå…¨ç½‘çƒ­ç‚¹ (è¡¥å……æœç´¢)
    {
        "name": "ç½‘ç»œçƒ­ç‚¹",
        "keywords": ["æ–‡å­¦æ‰¹è¯„", "ä½œå®¶ä¸“è®¿", "ç½‘ç»œæ–‡å­¦ æ’è¡Œæ¦œ"],
        "extra_query": ""
    }
]

def fetch_zone_news(zone):
    print(f"æ­£åœ¨æ‰«ææˆ˜åŒºï¼š[{zone['name']}] ...")
    zone_pool = []
    
    for kw in zone['keywords']:
        # æ ¸å¿ƒä¿®æ”¹ï¼šrtt=1 å¼ºåˆ¶ç™¾åº¦æŒ‰æ—¶é—´æ’åº (Real-time)
        query = kw + zone['extra_query']
        url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd={query}"
        
        try:
            res = requests.get(url, headers=get_header(), timeout=12)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            items = soup.find_all('div', class_='result-op')
            if not items: items = soup.find_all('div', class_='result')
            
            for item in items:
                try:
                    title_tag = item.find('h3').find('a')
                    title = title_tag.get_text(strip=True)
                    link = title_tag['href']
                    
                    source_node = item.find('span', class_='c-color-gray')
                    source = source_node.get_text(strip=True) if source_node else zone['name']
                    
                    time_node = item.find('span', class_='c-color-gray2')
                    time_str = time_node.get_text(strip=True) if time_node else ""

                    # --- å…³é”®æ­¥éª¤ï¼šæ—¶é—´æ¸…æ´— ---
                    # 1. ç®—å‡ºå…·ä½“æ—¶é—´
                    real_time = parse_baidu_time(time_str)
                    # 2. ç®—å‡ºæ˜¯å‡ å¤©å‰çš„
                    days_diff = (datetime.datetime.now() - real_time).days
                    
                    # 3. è¿™é‡Œçš„é€»è¾‘ï¼šå¦‚æœè¶…è¿‡ 2 å¤©ï¼Œç›´æ¥æ‰”æ‰ï¼
                    if days_diff > MAX_DAYS_AGO:
                        continue

                    category = auto_classify(title)
                    
                    # å»é‡å¹¶åŠ å…¥åˆ—è¡¨
                    if not any(n['title'] == title for n in zone_pool):
                        zone_pool.append({
                            "title": title, "url": link, "source": source, 
                            "time": time_str,     # æ˜¾ç¤ºç”¨çš„æ—¶é—´å­—ç¬¦ä¸²
                            "timestamp": real_time, # æ’åºç”¨çš„æ—¶é—´å¯¹è±¡
                            "category": category
                        })
                except:
                    continue
            time.sleep(1) # ç¤¼è²Œå»¶æ—¶
        except Exception as e:
            print(f"  æœç´¢[{kw}]å‡ºé”™: {e}")

    return zone_pool

def fetch_all():
    all_news = []
    # 1. æ‰«ææ‰€æœ‰æˆ˜åŒº
    for zone in SEARCH_ZONES:
        news = fetch_zone_news(zone)
        all_news.extend(news)
    
    # 2. è¡¥å……å¾ˆéš¾çˆ¬å–çš„å¹³å°ï¼ˆçŸ¥ç½‘ã€å¾®ä¿¡ï¼‰ä¸ºå›ºå®šç›´è¾¾å…¥å£
    # è¿™äº›å¹³å°åçˆ¬è™«æä¸¥ï¼Œç›´æ¥çˆ¬ä¼šå°IPï¼Œç”¨â€œç›´è¾¾æœç´¢é“¾æ¥â€æ˜¯æœ€ä¼˜è§£
    now = datetime.datetime.now()
    static_links = [
        {"title": "ğŸ‘‰ã€å¾®ä¿¡æœç‹—ã€‘â€œæ–‡å­¦è¯„è®ºâ€å…¬ä¼—å·æœ€æ–°æ–‡ç«  (ç‚¹å‡»ç›´è¾¾)", "url": "https://weixin.sogou.com/weixin?type=2&query=æ–‡å­¦è¯„è®º", "source": "å¾®ä¿¡çŸ©é˜µ", "time": "å®æ—¶", "timestamp": now, "category": "voice"},
        {"title": "ğŸ‘‰ã€çŸ¥ç½‘ã€‘â€œæ•°å­—äººæ–‡â€æœ€æ–°å­¦æœ¯è®ºæ–‡ (æŒ‰æ—¶é—´æ’åº)", "url": "https://scholar.baidu.com/scholar?q=æ•°å­—äººæ–‡&sc_ylo=2024&as_ylo=2025&sort=sc_time", "source": "CNKI/å­¦æœ¯", "time": "å®æ—¶", "timestamp": now, "category": "meeting"},
        {"title": "ğŸ‘‰ã€Bç«™ã€‘æ–‡å­¦è®²åº§æœ€æ–°è§†é¢‘å®å½•", "url": "https://search.bilibili.com/all?keyword=æ–‡å­¦è®²åº§&order=pubdate", "source": "Bilibili", "time": "å®æ—¶", "timestamp": now, "category": "activity"},
    ]
    
    # 3. åˆå¹¶æ•°æ®
    final_list = static_links + all_news
    
    # 4. å†æ¬¡æŒ‰æ ‡é¢˜å»é‡
    seen = set()
    unique_list = []
    for item in final_list:
        if item['title'] not in seen:
            unique_list.append(item)
            seen.add(item['title'])

    # 5. æœ€ç»ˆæ’åºï¼šæŒ‰æ—¶é—´å€’åºï¼ˆæœ€æ–°çš„æ’æœ€å‰é¢ï¼‰ï¼ï¼ï¼
    unique_list.sort(key=lambda x: x['timestamp'], reverse=True)
    
    # 6. åˆ é™¤ timestamp å­—æ®µï¼ˆä¸éœ€è¦ä¼ ç»™å‰ç«¯ï¼‰
    for item in unique_list:
        del item['timestamp']
        
    return unique_list[:60] # åªä¿ç•™æœ€æ–°çš„60æ¡

def save(data):
    try:
        # è°ƒæ•´ä¸ºåŒ—äº¬æ—¶é—´æ˜¾ç¤º
        utc_now = datetime.datetime.utcnow()
        cst_now = utc_now + datetime.timedelta(hours=8)
        time_str = cst_now.strftime('%Y-%m-%d %H:%M')
        
        # å˜é‡åä¿æŒ LIT_DATAï¼Œä¸ç”¨æ”¹ HTML
        final_json = {
            "update_time": time_str,
            "news": data
        }
        
        with open("data.js", "w", encoding="utf-8") as f:
            f.write(f"window.LIT_DATA = {json.dumps(final_json, ensure_ascii=False, indent=2)};")
            
        print("-" * 30)
        print(f"âœ… æŠ“å–å®Œæˆï¼æ—¶é—´: {time_str}ï¼Œå…± {len(data)} æ¡æ–°é²œèµ„è®¯")
        
    except Exception as e:
        print(f"Save Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    data = fetch_all()
    save(data)
