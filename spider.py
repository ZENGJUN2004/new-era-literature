import requests
from bs4 import BeautifulSoup
import json
import time
import random
import sys
import io

# 1. å¼ºåˆ¶è®¾ç½®è¾“å‡ºç¼–ç ï¼Œé˜²æ­¢äº‘ç«¯æ‰“å°ä¸­æ–‡/Emojiæ—¶å´©æºƒ
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# 2. æœç´¢å…³é”®è¯æ±  (å¯éšæ—¶æ·»åŠ )
KEYWORDS = [
    "ä¸­å›½ä½œå", "é²è¿…æ–‡å­¦å¥–", "èŒ…ç›¾æ–‡å­¦å¥–", "æ–°ä¹¦å‘å¸ƒä¼š", 
    "æ–‡å­¦ç ”è®¨ä¼š", "ä½œå®¶ä¸“è®¿", "æ–‡å­¦è¯„è®º", "ç½‘ç»œæ–‡å­¦", 
    "è«è¨€", "ä½™å", "ç‹å®‰å¿†", "è´¾å¹³å‡¹"
]

def get_header():
    # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.baidu.com/'
    }

def auto_classify(title):
    """æ™ºèƒ½åˆ†ç±»ç®—æ³•"""
    t = title.lower()
    # ä¼˜å…ˆçº§ 1: ä¼šè®®
    if any(k in t for k in ['ç ”è®¨', 'åº§è°ˆ', 'è®ºå›', 'å³°ä¼š', 'å¹´ä¼š', 'ä¼šè®®', 'è‡´è¾', 'å¼€å¹•']):
        return 'meeting'
    # ä¼˜å…ˆçº§ 2: å£°éŸ³ (è§‚ç‚¹/è®¿è°ˆ)
    if any(k in t for k in ['ä¸“è®¿', 'å¯¹è¯', 'è°ˆ', 'è¯´', 'è®º', 'åºè¨€', 'è¯»åæ„Ÿ', 'æ‰¹è¯„', 'è§‚ç‚¹']):
        return 'voice'
    # ä¼˜å…ˆçº§ 3: æ´»åŠ¨ (å‘å¸ƒ/å¥–é¡¹)
    if any(k in t for k in ['å‘å¸ƒ', 'æ­æ™“', 'é¢å¥–', 'å¯åŠ¨', 'å¾æ–‡', 'å¤§èµ›', 'è®²åº§', 'æ´»åŠ¨']):
        return 'activity'
    return 'other'

def fetch_literary_news():
    news_pool = []
    print(">>> å¼€å§‹æŠ“å–æ–‡å­¦èµ„è®¯...")
    
    for kw in KEYWORDS:
        print(f"Searching: {kw}") 
        url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd={kw}"
        
        try:
            res = requests.get(url, headers=get_header(), timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # å…¼å®¹ç™¾åº¦ä¸åŒçš„ç»“æ„
            items = soup.find_all('div', class_='result-op')
            if not items: items = soup.find_all('div', class_='result')
            
            for item in items:
                try:
                    title_tag = item.find('h3').find('a')
                    title = title_tag.get_text(strip=True)
                    link = title_tag['href']
                    source = item.find('span', class_='c-color-gray').get_text(strip=True)
                    pub_time = item.find('span', class_='c-color-gray2').get_text(strip=True)
                    
                    category = auto_classify(title)
                    
                    # å»é‡
                    if not any(n['title'] == title for n in news_pool):
                        news_pool.append({
                            "title": title, "url": link, "source": source, 
                            "time": pub_time, "category": category
                        })
                except:
                    continue
        except Exception as e:
            print(f"Error on {kw}: {e}")
        
        time.sleep(1) # ç¤¼è²Œå»¶æ—¶

    # 3. æ’å…¥ç¤¾äº¤åª’ä½“ç½®é¡¶å…¥å£
    social_links = [
        {"title": "ğŸ‘‰ã€å¾®åšã€‘ä¸­å›½ä½œå®¶åä¼š - å®˜æ–¹å®æ—¶åŠ¨æ€", "url": "https://s.weibo.com/weibo?q=ä¸­å›½ä½œå", "source": "å¾®åš", "time": "å®æ—¶", "category": "meeting"},
        {"title": "ğŸ‘‰ã€æŠ–éŸ³ã€‘æœç´¢â€œæ–°ä¹¦å‘å¸ƒä¼šâ€ç°åœºè§†é¢‘", "url": "https://www.douyin.com/search/æ–°ä¹¦å‘å¸ƒä¼š", "source": "æŠ–éŸ³", "time": "å®æ—¶", "category": "activity"},
        {"title": "ğŸ‘‰ã€å°çº¢ä¹¦ã€‘æœç´¢â€œæ–‡å­¦æ‰¹è¯„â€æœ€æ–°ç¬”è®°", "url": "https://www.xiaohongshu.com/search_result?keyword=æ–‡å­¦æ‰¹è¯„", "source": "å°çº¢ä¹¦", "time": "å®æ—¶", "category": "voice"},
    ]
    
    # ç¤¾äº¤åœ¨å‰ï¼Œæ–°é—»åœ¨å
    return social_links + news_pool[:60]

def save(data):
    try:
        # æ³¨æ„ï¼šè¿™é‡Œç”Ÿæˆçš„å˜é‡åæ˜¯ LIT_DATA
        final_json = {
            "update_time": time.strftime("%Y-%m-%d %H:%M", time.localtime()),
            "news": data
        }
        with open("data.js", "w", encoding="utf-8") as f:
            f.write(f"window.LIT_DATA = {json.dumps(final_json, ensure_ascii=False, indent=2)};")
        print(f"Success! Saved {len(data)} items.")
    except Exception as e:
        print(f"Save Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    try:
        data = fetch_literary_news()
        save(data)
    except Exception as e:
        print(f"Critical Script Error: {e}")
        # è¿™é‡Œä¸é€€å‡ºï¼Œé˜²æ­¢GitHubæŠ¥çº¢ï¼Œè‡³å°‘ä¿è¯æµç¨‹è·‘é€š