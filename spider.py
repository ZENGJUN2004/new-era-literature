import requests
from bs4 import BeautifulSoup
import json
import time
import random
import sys
import io
import datetime
import re

# 1. åŸºç¡€ç¯å¢ƒ
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
MAX_DAYS_AGO = 2  # åªçœ‹æœ€è¿‘ 2 å¤©

def get_header():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.baidu.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }

# 2. æ—¶é—´æ¸…æ´—å™¨ (ä¿æŒä¸¥å‰çš„è¿‡æ»¤é€»è¾‘)
def parse_baidu_time(time_str):
    now = datetime.datetime.now()
    time_str = str(time_str).strip()
    try:
        if "åˆ†é’Ÿå‰" in time_str:
            mins = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(minutes=mins)
        elif "å°æ—¶å‰" in time_str:
            hours = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(hours=hours)
        elif "æ˜¨å¤©" in time_str:
            return now - datetime.timedelta(days=1)
        elif "å¤©å‰" in time_str:
            days = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(days=days)
        elif "å¹´" in time_str or "-" in time_str:
            clean_str = time_str.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            return datetime.datetime.strptime(clean_str, "%Y-%m-%d")
        else:
            return now # é»˜è®¤ç®—æœ€æ–°
    except:
        return now - datetime.timedelta(days=365)

# 3. æ™ºèƒ½å†…å®¹åˆ†ç±» (æŒ‰ç”¨æˆ·éœ€æ±‚é‡æ–°å®šä¹‰)
def auto_classify(title):
    t = title.lower()
    # æ ¸å¿ƒå…³æ³¨ï¼šä½œå“ä¸ä½œå®¶
    if any(k in t for k in ['æ–°ä¹¦', 'é¦–å‘', 'å°è¯´', 'ç›®å½•', 'ä¸Šå¸‚', 'å‡ºç‰ˆ', 'è¿è½½', 'é€‰è½½']):
        return 'activity' # è¿™é‡Œå¯¹åº”å‰ç«¯çš„â€œæ´»åŠ¨/åŠ¨æ€â€ï¼Œå®é™…æŒ‡â€œä½œå“åŠ¨æ€â€
    # æ ¸å¿ƒå…³æ³¨ï¼šæ€æ½®ä¸è¯é¢˜
    if any(k in t for k in ['æ€æ½®', 'çƒ­ç‚¹', 'äº‰è®®', 'ç°è±¡', 'éè™šæ„', 'ç§‘å¹»', 'å¥³æ€§å†™ä½œ', 'aiå†™ä½œ', 'æ’è¡Œæ¦œ']):
        return 'voice'    # è¿™é‡Œå¯¹åº”å‰ç«¯çš„â€œå£°éŸ³/è§‚ç‚¹â€ï¼Œå®é™…æŒ‡â€œè¯é¢˜â€
    # æ ¸å¿ƒå…³æ³¨ï¼šæ‰¹è¯„ä¸ç ”ç©¶
    if any(k in t for k in ['è¯„è®º', 'æ‰¹è¯„', 'ç ”è®¨', 'ç»¼è¿°', 'è®º', 'è¯»åæ„Ÿ', 'ç¬”è°ˆ', 'è®²åº§']):
        return 'meeting'  # è¿™é‡Œå¯¹åº”å‰ç«¯çš„â€œä¼šè®®/å­¦æœ¯â€ï¼Œå®é™…æŒ‡â€œç ”ç©¶â€
    return 'other'

# ==========================================
# 4. å…¨æ–°æˆ˜ç•¥ï¼šä¸‰å¤§å†…å®¹æˆ˜åŒº
# ==========================================
SEARCH_ZONES = [
    # æˆ˜åŒºAï¼šçŸ¥åä½œå®¶ä¸é‡ç£…ä½œå“ (ç›¯ç€â€œè°å‡ºäº†ä»€ä¹ˆä¹¦â€)
    {
        "name": "æ–°ä½œé¦–å‘",
        "keywords": [
            "é•¿ç¯‡å°è¯´ é¦–å‘", "ä½œå®¶ æ–°ä¹¦å‘å¸ƒ", "æ–‡å­¦æœŸåˆŠ ç›®å½•", 
            "ã€Šæ”¶è·ã€‹ç›®å½•", "ã€Šåæœˆã€‹æ‚å¿—", "ã€Šäººæ°‘æ–‡å­¦ã€‹", 
            "èŒ…ç›¾æ–‡å­¦å¥– ä½œå®¶", "é²è¿…æ–‡å­¦å¥– å¾—ä¸» æ–°ä¹¦"
        ]
    },
    # æˆ˜åŒºBï¼šæ–‡å­¦æ€æ½®ä¸çƒ­ç‚¹è¯é¢˜ (ç›¯ç€â€œåœˆé‡Œåœ¨åµä»€ä¹ˆâ€)
    {
        "name": "æ€æ½®çƒ­ç‚¹",
        "keywords": [
            "æ–‡å­¦åœˆ çƒ­ç‚¹", "æ–‡å­¦ äº‰è®®", "éè™šæ„å†™ä½œ è®¨è®º", 
            "ç§‘å¹»æ–‡å­¦ è¶‹åŠ¿", "å¥³æ€§æ–‡å­¦ è¯é¢˜", "å½“ä»£æ–‡å­¦ ç°è±¡",
            "è±†ç“£è¯»ä¹¦ é«˜åˆ†", "æ–‡å­¦å¹´åº¦æ¦œå•"
        ]
    },
    # æˆ˜åŒºCï¼šæ‰¹è¯„å®¶ä¸æ·±åº¦è§‚å¯Ÿ (ç›¯ç€â€œä¸“å®¶æ€ä¹ˆçœ‹â€)
    {
        "name": "æ‰¹è¯„äº‰é¸£",
        "keywords": [
            "æ–‡å­¦è¯„è®ºå®¶ å‘å£°", "æ–‡å­¦ç ”è®¨ä¼š ç»¼è¿°", "å½“ä»£æ–‡å­¦æ‰¹è¯„", 
            "é™ˆæ™“æ˜ æ–‡å­¦", "æˆ´é”¦å è®¿è°ˆ", "ææ•¬æ³½ è§‚ç‚¹", # ä¸¾ä¾‹å‡ ä½æ´»è·ƒçš„æ‰¹è¯„å®¶
            "ä¸­å›½å½“ä»£æ–‡å­¦ç ”ç©¶ä¼š", "å­¦æœ¯æœˆåˆŠ æ–‡å­¦"
        ]
    }
]

def fetch_zone_news(zone):
    print(f"æ­£åœ¨æ·±æŒ–å†…å®¹ï¼š[{zone['name']}] ...")
    zone_pool = []
    
    for kw in zone['keywords']:
        # rtt=1 å¼ºåˆ¶æŒ‰æ—¶é—´æ’åº
        url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd={kw}"
        
        try:
            res = requests.get(url, headers=get_header(), timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            items = soup.find_all('div', class_='result-op')
            if not items: items = soup.find_all('div', class_='result')
            
            for item in items:
                try:
                    title_tag = item.find('h3').find('a')
                    title = title_tag.get_text(strip=True)
                    link = title_tag['href']
                    source = item.find('span', class_='c-color-gray').get_text(strip=True) if item.find('span', class_='c-color-gray') else "æ–‡å­¦ç°åœº"
                    time_str = item.find('span', class_='c-color-gray2').get_text(strip=True) if item.find('span', class_='c-color-gray2') else ""

                    # ä¸¥æ ¼çš„æ—¶é—´è¿‡æ»¤
                    real_time = parse_baidu_time(time_str)
                    if (datetime.datetime.now() - real_time).days > MAX_DAYS_AGO:
                        continue

                    category = auto_classify(title)
                    
                    if not any(n['title'] == title for n in zone_pool):
                        zone_pool.append({
                            "title": title, "url": link, "source": source, 
                            "time": time_str,
                            "timestamp": real_time,
                            "category": category
                        })
                except: continue
            time.sleep(1)
        except Exception as e:
            print(f"  [{kw}] æœç´¢ä¸­æ–­: {e}")

    return zone_pool

def fetch_all():
    all_news = []
    for zone in SEARCH_ZONES:
        all_news.extend(fetch_zone_news(zone))
    
    # --- å…³é”®è¡¥å……ï¼šé’ˆå¯¹å¾ˆéš¾çˆ¬å–çš„â€œæ·±åº¦å†…å®¹â€æä¾›ç›´è¾¾æ¢¯å­ ---
    now = datetime.datetime.now()
    static_links = [
        {"title": "ğŸ‘‰ã€å¾®ä¿¡æ·±åº¦ã€‘æœç´¢â€œæ–‡å­¦æ‰¹è¯„â€å…¬ä¼—å·æœ€æ–°æ–‡ç« ", "url": "https://weixin.sogou.com/weixin?type=2&query=æ–‡å­¦æ‰¹è¯„", "source": "å¾®ä¿¡", "time": "å®æ—¶", "timestamp": now, "category": "voice"},
        {"title": "ğŸ‘‰ã€è±†ç“£è¯»ä¹¦ã€‘æœ¬å‘¨è™šæ„ç±»çƒ­é—¨å›¾ä¹¦æ¦œ", "url": "https://book.douban.com/chart?subcat=F", "source": "è±†ç“£", "time": "æœ¬å‘¨", "timestamp": now, "category": "activity"},
        {"title": "ğŸ‘‰ã€çŸ¥ç½‘å­¦æœ¯ã€‘â€œå½“ä»£æ–‡å­¦â€æœ€æ–°æ ¸å¿ƒæœŸåˆŠè®ºæ–‡", "url": "https://scholar.baidu.com/scholar?q=å½“ä»£æ–‡å­¦&sc_ylo=2024&sort=sc_time", "source": "CNKI", "time": "å®æ—¶", "timestamp": now, "category": "meeting"},
    ]
    
    final_list = static_links + all_news
    
    # å»é‡
    seen = set()
    unique_list = []
    for item in final_list:
        if item['title'] not in seen:
            unique_list.append(item)
            seen.add(item['title'])

    # æŒ‰æ—¶é—´å€’åº
    unique_list.sort(key=lambda x: x['timestamp'], reverse=True)
    
    # æ¸…ç†å­—æ®µ
    for item in unique_list:
        del item['timestamp']
        
    return unique_list[:60]

def save(data):
    try:
        # åŒ—äº¬æ—¶é—´
        utc_now = datetime.datetime.utcnow()
        cst_now = utc_now + datetime.timedelta(hours=8)
        time_str = cst_now.strftime('%Y-%m-%d %H:%M')
        
        final_json = { "update_time": time_str, "news": data }
        
        with open("data.js", "w", encoding="utf-8") as f:
            f.write(f"window.LIT_DATA = {json.dumps(final_json, ensure_ascii=False, indent=2)};")
        print(f"âœ… å†…å®¹æŠ“å–å®Œæˆï¼Œå…± {len(data)} æ¡")
    except Exception as e:
        sys.exit(1)

if __name__ == "__main__":
    data = fetch_all()
    save(data)

